{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = 'u/ebanner/Classes/nlp/Project/irony-classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = 'u/npockrus/NLP/finalProject/venv/src/irony-classifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Conservative Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/v/filer4b/v20q001/ebanner/Classes/nlp/Project/irony-classifier/data/progressive/features\n"
     ]
    }
   ],
   "source": [
    "cd /{base}/data/progressive/features/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "with open('text-sentiment-label.p', 'r') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Load comments, labels, and bow vectors\n",
    "xs = np.array([ comment for comment in data ])\n",
    "sentiments = np.array([ data[comment]['sentiment'] for comment in data ])\n",
    "ys = np.array([ data[comment]['label'] for comment in data ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crunch Down the Data Just For Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only take the first ten positive and negative training example\n",
    "plusses = [ (x, sentiment, y) for x, sentiment, y in zip(xs, sentiments, ys) if y ==  1 ][:10]\n",
    "minuses = [ (x, sentiment, y) for x, sentiment, y in zip(xs, sentiments, ys) if y == -1 ][:10]\n",
    "\n",
    "# Extract the plusses back out\n",
    "plus_xs = [ x for x, sentiment, y in plusses ]\n",
    "plus_sentiments = [ sentiment for x, sentiment, y in plusses ]\n",
    "plus_ys = [ y for x, sentiment, y in plusses ]\n",
    "\n",
    "# Extract the minuses back out\n",
    "minus_xs = [ x for x, sentiment, y in minuses ]\n",
    "minus_sentiments = [ sentiment for x, sentiment, y in minuses ]\n",
    "minus_ys = [ y for x, sentiment, y in minuses ]\n",
    "\n",
    "# Put everything back together so we have ten plusses followed by ten minuses\n",
    "xs = np.array(plus_xs + minus_xs)\n",
    "ys = np.array(plus_ys + minus_ys)\n",
    "sentiments = np.array(plus_sentiments + minus_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "kf = KFold(len(xs), n_folds=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build N-Gram Language Model on the Train Data and Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/v/filer4b/v20q001/ebanner/Classes/nlp/Project/irony-classifier/lib/berkeleylm-1.1.5/examples\n"
     ]
    }
   ],
   "source": [
    "cd /{base}/lib/berkeleylm-1.1.5/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text files [genuine-progressive.txt] and writing to file genuine-progressive.arpa {\n",
      "\tReading in ngrams from raw text {\n",
      "\t\tOn line 0\n",
      "\t} [0s]\n",
      "\tWriting Kneser-Ney probabilities {\n",
      "\t\tCounting counts for order 0 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 1 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 2 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 3 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 4 {\n",
      "\t\t} [0s]\n",
      "\t\tWriting ARPA {\n",
      "\t\t\tOn order 1\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 2\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tWriting line 10001\n",
      "\t\t\tWriting line 20001\n",
      "\t\t\tOn order 3\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tWriting line 10001\n",
      "\t\t\tWriting line 20001\n",
      "\t\t\tOn order 4\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tWriting line 10001\n",
      "\t\t\tWriting line 20001\n",
      "\t\t\tOn order 5\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tWriting line 10001\n",
      "\t\t\tWriting line 20001\n",
      "\t\t} [0s]\n",
      "\t} [0s]\n",
      "Finish building progressive ARPA from text!\n",
      "Reading Lm File genuine-progressive.arpa . . .  {\n",
      "\tCounting values {\n",
      "\t\tParsing ARPA language model file {\n",
      "\t\t\tReading 1-grams {\n",
      "\t\t\t\tRead 0 lines\n",
      "\t\t\t\t6936 1-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 2-grams {\n",
      "\t\t\t\t20018 2-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 3-grams {\n",
      "\t\t\t\t23983 3-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 4-grams {\n",
      "\t\t\t\t194 4-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 5-grams {\n",
      "\t\t\t} [0s]\n",
      "\t\t} [0s]\n",
      "\t\tCleaning up values {\n",
      "\t\t\tFound 14925 unique counts\n",
      "\t\t} [0s]\n",
      "\t} [0s]\n",
      "\tStoring values {\n",
      "\t\tStoring count indices using 14 bits.\n",
      "\t} [0s]\n",
      "\tAdding n-grams {\n",
      "\t\tParsing ARPA language model file {\n",
      "\t\t\tReading 1-grams {\n",
      "\t\t\t\tRead 0 lines\n",
      "\t\t\t\t6936 1-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 2-grams {\n",
      "\t\t\t\t20018 2-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 3-grams {\n",
      "\t\t\t\t23983 3-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 4-grams {\n",
      "\t\t\t\t194 4-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 5-grams {\n",
      "\t\t\t} [0s]\n",
      "\t\t} [0s]\n",
      "\t} [0s]\n",
      "\t4 missing suffixes or prefixes were found, doing another pass to add n-grams {\n",
      "\t\tParsing ARPA language model file {\n",
      "\t\t\tReading 1-grams {\n",
      "\t\t\t\tRead 0 lines\n",
      "\t\t\t\t6936 1-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 2-grams {\n",
      "\t\t\t\t20018 2-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 3-grams {\n",
      "\t\t\t\t23983 3-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 4-grams {\n",
      "\t\t\t\t194 4-gram read.\n",
      "\t\t\t} [0s]\n",
      "\t\t\tReading 5-grams {\n",
      "\t\t\t} [0s]\n",
      "\t\t} [0s]\n",
      "\t\tLoad factor for 1: 1.0\n",
      "\t\tLoad factor for 2: 0.7583724806788907\n",
      "\t\tLoad factor for 3: 0.7252850273686757\n",
      "\t\tLoad factor for 4: 0.8048780487804879\n",
      "\t\tLoad factor for 5: 0.9240506329113924\n",
      "\t} [0s]\n",
      "} [0s]\n",
      "Writing to file genuine-progressive.binary . . .  {\n",
      "} [0s]\n",
      "Must get to: 1825\n",
      "0\n",
      "['Log probability of text is: -4.301421165466309']\n",
      "1\n",
      "['Log probability of text is: -4.357237339019775']\n",
      "2\n",
      "['Log probability of text is: -100.0']\n",
      "3\n",
      "['Log probability of text is: -4.585716247558594']\n",
      "4\n",
      "['Log probability of text is: -4.301421165466309']\n",
      "5\n",
      "['Log probability of text is: -4.533328056335449']\n",
      "6\n",
      "['Log probability of text is: -4.301421165466309']\n",
      "7\n",
      "['Log probability of text is: -4.415229320526123']\n",
      "8\n",
      "['Log probability of text is: -4.301421165466309']\n",
      "9\n",
      "['Log probability of text is: -4.301421165466309']\n",
      "10\n",
      "['Log probability of text is: -4.556808948516846']\n",
      "11\n",
      "['Log probability of text is: -4.489861965179443']\n",
      "12\n",
      "['Log probability of text is: -4.357237339019775']\n",
      "13\n",
      "['Log probability of text is: -4.301421165466309']\n",
      "14\n",
      "['Log probability of text is: -4.301421165466309']\n",
      "15\n",
      "['Log probability of text is: -4.833596229553223']\n",
      "16\n",
      "['Log probability of text is: -4.415229320526123']\n",
      "17\n",
      "['Log probability of text is: -4.301421165466309']\n",
      "18\n",
      "['Log probability of text is: -3.6993610858917236']\n",
      "19\n",
      "['Log probability of text is: -4.301421165466309']\n",
      "20\n",
      "['Log probability of text is: -4.301421165466309']\n",
      "21\n",
      "['Log probability of text is: -4.470311164855957']\n",
      "22\n",
      "['Log probability of text is: -4.357237339019775']\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "precisions, recalls, f_measures = [], [], []\n",
    "for train, test in kf:\n",
    "#     print train, test\n",
    "#     # Filter down to just genuine comments and tokenize them\n",
    "#     genuine_sentences = [ x for x, y in zip(xs[train], ys[train]) if y == -1 ]\n",
    "#     genuine_sentences = [ ' '.join(word_tokenize(genuine_sentence)) for genuine_sentence in genuine_sentences ]\n",
    "\n",
    "    genuine_sentences = xs[train]\n",
    "    \n",
    "    # Write the genuine tokenized comments to disk so the Berkeley N-Gram Language model can be trained\n",
    "    with open('genuine-progressive.txt', 'w') as f:\n",
    "        for genuine_sentence in genuine_sentences:\n",
    "            f.write(genuine_sentence.encode('utf-8') + '\\n')\n",
    "    \n",
    "    # Make an arpa straight from text\n",
    "    !java -ea -mx1000m -server -cp ../src edu.berkeley.nlp.lm.io.MakeKneserNeyArpaFromText 5 genuine-progressive.arpa genuine-progressive.txt\n",
    "    print 'Finish building progressive ARPA from text!'\n",
    "    \n",
    "    # Make a binary from the arpa\n",
    "    !java -ea -mx1000m -server -cp ../src edu.berkeley.nlp.lm.io.MakeLmBinaryFromArpa genuine-progressive.arpa genuine-progressive.binary\n",
    "    \n",
    "    # Extract train probabilities to train a classifier\n",
    "    probs = [0]*len(xs[train])\n",
    "    print 'Must get to: {}'.format(len(xs[train]))\n",
    "    for i, sentence in enumerate(xs[train]):\n",
    "        print i\n",
    "        sentence = ' '.join(word_tokenize(sentence))\n",
    "        out = !echo \"{sentence}\" | java -ea -mx1000m -server -cp ../src edu.berkeley.nlp.lm.io.ComputeLogProbabilityOfTextStream genuine-progressive.binary 2>&1 | tail -n 1\n",
    "        print out\n",
    "        prob = float(out[0].split()[5])\n",
    "        probs[i] = prob\n",
    "    print 'Finish extracting conservativiness for training!'\n",
    "    \n",
    "    # Train a simple classifier that just uses sentiment*probabilities\n",
    "    svm = SGDClassifier(loss=\"hinge\", penalty=\"l2\", class_weight=\"auto\")\n",
    "    parameters = { 'alpha': [.001, .01,  .1] }\n",
    "    clf = GridSearchCV(svm, parameters, scoring='f1')\n",
    "    conservativinesses = np.array([ [prob*sentiment] for prob, sentiment in zip(probs, sentiments[train]) ])\n",
    "    clf.fit(conservativinesses, ys[train])\n",
    "    print 'Trained the classifier on conservativiness!'\n",
    "    \n",
    "    # Extract the probabilities for the test set\n",
    "    probs = [0]*len(xs[test])\n",
    "    print 'Must get to: {}'.format(len(xs[test]))\n",
    "    for i, sentence in enumerate(xs[test]):\n",
    "        print i\n",
    "        sentence = ' '.join(word_tokenize(sentence))\n",
    "        out = !echo \"{sentence}\" | java -ea -mx1000m -server -cp ../src edu.berkeley.nlp.lm.io.ComputeLogProbabilityOfTextStream progressive.binary 2>&1 | tail -n 1\n",
    "        prob = float(out[0].split()[5])\n",
    "        probs[i] = prob\n",
    "    print 'Finish extracting conservativiness for test!'\n",
    "    \n",
    "    # Make predictions\n",
    "    conservativinesses = np.array([ [prob*sentiment] for prob, sentiment in zip(probs, sentiments[test]) ])\n",
    "    predictions = clf.predict(conservativinesses)\n",
    "    \n",
    "    # Record statistics\n",
    "    precision, recall, f_measure, _ = sklearn.metrics.precision_recall_fscore_support(ys[test], predictions, average='binary')\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f_measures.append(f_measure)\n",
    "    \n",
    "    print 'Precision: {}'.format(precision)\n",
    "    print 'Recall: {}'.format(precision)\n",
    "    print 'F-Measure: {}'.format(precision)\n",
    "    \n",
    "print 'Precisions: {}'.format(precisions)\n",
    "print 'Recalls: {}'.format(recalls)\n",
    "print 'F-Measures: {}'.format(f_measures)\n",
    "print\n",
    "print 'Mean Precision: {}'.format(np.mean(precisions))\n",
    "print 'Mean Recall: {}'.format(np.mean(recalls))\n",
    "print 'Mean F-Measure: {}'.format(np.mean(f_measures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09327809])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = clf.best_estimator_\n",
    "sgd_clf.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Precision: 0.0534009998485\n",
    "# Mean Recall: 0.65406162465\n",
    "# Mean F-Measure: 0.0902528618887"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
