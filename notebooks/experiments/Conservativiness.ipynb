{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/v/filer4b/v20q001/ebanner/Classes/nlp/Project/irony-classifier/data/conservative/preprocess\n"
     ]
    }
   ],
   "source": [
    "cd /v/filer4b/v20q001/ebanner/Classes/nlp/Project/irony-classifier/data/conservative/preprocess/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Conservative Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "with open('text-sentiment-label.p', 'r') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Load comments, labels, and bow vectors\n",
    "xs = np.array([ comment for comment in data ])\n",
    "sentiments = np.array([ data[comment]['sentiment'] for comment in data ])\n",
    "ys = np.array([ data[comment]['label'] for comment in data ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crunch Down the Data Just For Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only take the first ten positive and negative training example\n",
    "plusses = [ (x, sentiment, y) for x, sentiment, y in zip(xs, sentiments, ys) if y ==  1 ][:10]\n",
    "minuses = [ (x, sentiment, y) for x, sentiment, y in zip(xs, sentiments, ys) if y == -1 ][:10]\n",
    "\n",
    "# Extract the plusses back out\n",
    "plus_xs = [ x for x, sentiment, y in plusses ]\n",
    "plus_sentiments = [ sentiment for x, sentiment, y in plusses ]\n",
    "plus_ys = [ y for x, sentiment, y in plusses ]\n",
    "\n",
    "# Extract the minuses back out\n",
    "minus_xs = [ x for x, sentiment, y in minuses ]\n",
    "minus_sentiments = [ sentiment for x, sentiment, y in minuses ]\n",
    "minus_ys = [ y for x, sentiment, y in minuses ]\n",
    "\n",
    "# Put everything back together so we have ten plusses followed by ten minuses\n",
    "xs = np.array(plus_xs + minus_xs)\n",
    "ys = np.array(plus_ys + minus_ys)\n",
    "sentiments = np.array(plus_sentiments + minus_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "kf = KFold(len(xs), n_folds=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build N-Gram Language Model on the Train Data and Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/v/filer4b/v20q001/ebanner/Classes/nlp/Project/irony-classifier/lib/berkeleylm-1.1.5/examples\n"
     ]
    }
   ],
   "source": [
    "cd /u/ebanner/Classes/nlp/Project/irony-classifier/lib/berkeleylm-1.1.5/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  5  6  7  8  9 10 15 16 17 18 19] [ 2  3  4 11 12 13 14]\n",
      "Reading text files [genuine-conservative.txt] and writing to file conservative-model.arpa {\n",
      "\tReading in ngrams from raw text {\n",
      "\t\tOn line 0\n",
      "\t} [0s]\n",
      "\tWriting Kneser-Ney probabilities {\n",
      "\t\tCounting counts for order 0 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 1 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 2 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 3 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 4 {\n",
      "\t\t} [0s]\n",
      "\t\tWriting ARPA {\n",
      "\t\t\tOn order 1\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 2\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 3\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 4\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 5\n",
      "\t\t\tWriting line 1\n",
      "\t\t} [0s]\n",
      "\t} [0s]\n",
      "Finish building Berekeley NGram language model!\n",
      "Must get to: 13\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "Finish extracting conservativiness for training!\n",
      "Trained the classifier on conservativiness!\n",
      "Must get to: 7\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Finish extracting conservativiness for test!\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-Measure: 0.0\n",
      "[ 2  3  4  5  6  7  9 11 12 13 14 15 18] [ 0  1  8 10 16 17 19]\n",
      "Reading text files [genuine-conservative.txt] and writing to file conservative-model.arpa {\n",
      "\tReading in ngrams from raw text {\n",
      "\t\tOn line 0\n",
      "\t} [0s]\n",
      "\tWriting Kneser-Ney probabilities {\n",
      "\t\tCounting counts for order 0 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 1 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 2 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 3 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 4 {\n",
      "\t\t} [0s]\n",
      "\t\tWriting ARPA {\n",
      "\t\t\tOn order 1\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 2\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 3\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 4\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 5\n",
      "\t\t\tWriting line 1\n",
      "\t\t} [0s]\n",
      "\t} [0s]\n",
      "Finish building Berekeley NGram language model!\n",
      "Must get to: 13\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/ebanner/.virtualenvs/lab-2.7/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "Finish extracting conservativiness for training!\n",
      "Trained the classifier on conservativiness!\n",
      "Must get to: 7\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Finish extracting conservativiness for test!\n",
      "Precision: 0.666666666667\n",
      "Recall: 0.666666666667\n",
      "F-Measure: 0.666666666667\n",
      "[ 0  1  2  3  4  8 10 11 12 13 14 16 17 19] [ 5  6  7  9 15 18]\n",
      "Reading text files [genuine-conservative.txt] and writing to file conservative-model.arpa {\n",
      "\tReading in ngrams from raw text {\n",
      "\t\tOn line 0\n",
      "\t} [0s]\n",
      "\tWriting Kneser-Ney probabilities {\n",
      "\t\tCounting counts for order 0 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 1 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 2 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 3 {\n",
      "\t\t} [0s]\n",
      "\t\tCounting counts for order 4 {\n",
      "\t\t} [0s]\n",
      "\t\tWriting ARPA {\n",
      "\t\t\tOn order 1\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 2\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 3\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 4\n",
      "\t\t\tWriting line 1\n",
      "\t\t\tOn order 5\n",
      "\t\t\tWriting line 1\n",
      "\t\t} [0s]\n",
      "\t} [0s]\n",
      "Finish building Berekeley NGram language model!\n",
      "Must get to: 14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Finish extracting conservativiness for training!\n",
      "Trained the classifier on conservativiness!\n",
      "Must get to: 6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Finish extracting conservativiness for test!\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-Measure: 0.0\n",
      "Precisions: [0.0, 0.66666666666666663, 0.0]\n",
      "Recalls: [0.0, 0.66666666666666663, 0.0]\n",
      "F-Measures: [0.0, 0.66666666666666663, 0.0]\n",
      "\n",
      "Mean Precision: 0.222222222222\n",
      "Mean Recall: 0.222222222222\n",
      "Mean F-Measure: 0.222222222222\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "precisions, recalls, f_measures = [], [], []\n",
    "for train, test in kf:\n",
    "    print train, test\n",
    "    # Filter down to just genuine comments and tokenize them\n",
    "    genuine_sentences = [ x for x, y in zip(xs[train], ys[train]) if y == -1 ]\n",
    "    genuine_sentences = [ ' '.join(word_tokenize(genuine_sentence)) for genuine_sentence in genuine_sentences ]\n",
    "    \n",
    "    # Write the genuine tokenized comments to disk so the Berkeley N-Gram Language model can be trained\n",
    "    with open('genuine-conservative.txt', 'w') as f:\n",
    "        for genuine_sentence in genuine_sentences:\n",
    "            f.write(genuine_sentence.encode('utf-8') + '\\n')\n",
    "    \n",
    "    # Train the language model on it\n",
    "    !java -ea -mx1000m -server -cp ../src edu.berkeley.nlp.lm.io.MakeKneserNeyArpaFromText 5 conservative-model.arpa genuine-conservative.txt\n",
    "    print 'Finish building Berekeley NGram language model!'\n",
    "    \n",
    "    # Extract train probabilities to train a classifier\n",
    "    probs = [0]*len(xs[train])\n",
    "    print 'Must get to: {}'.format(len(xs[train]))\n",
    "    for i, sentence in enumerate(xs[train]):\n",
    "        print i\n",
    "        sentence = ' '.join(word_tokenize(sentence))\n",
    "        out = !echo \"{sentence}\" | java -ea -mx1000m -server -cp ../src edu.berkeley.nlp.lm.io.ComputeLogProbabilityOfTextStream conservative.binary 2>&1 | tail -n 1\n",
    "        prob = float(out[0].split()[5])\n",
    "        probs[i] = prob\n",
    "    print 'Finish extracting conservativiness for training!'\n",
    "    \n",
    "    # Train a simple classifier that just uses sentiment*probabilities\n",
    "    svm = SGDClassifier(loss=\"hinge\", penalty=\"l2\", class_weight=\"auto\")\n",
    "    parameters = { 'alpha': [.001, .01,  .1] }\n",
    "    clf = GridSearchCV(svm, parameters, scoring='f1')\n",
    "    conservativinesses = np.array([ [prob*sentiment] for prob, sentiment in zip(probs, sentiments[train]) ])\n",
    "    clf.fit(conservativinesses, ys[train])\n",
    "    print 'Trained the classifier on conservativiness!'\n",
    "    \n",
    "    # Extract the probabilities for the test set\n",
    "    probs = [0]*len(xs[test])\n",
    "    print 'Must get to: {}'.format(len(xs[test]))\n",
    "    for i, sentence in enumerate(xs[test]):\n",
    "        print i\n",
    "        sentence = ' '.join(word_tokenize(sentence))\n",
    "        out = !echo \"{sentence}\" | java -ea -mx1000m -server -cp ../src edu.berkeley.nlp.lm.io.ComputeLogProbabilityOfTextStream conservative.binary 2>&1 | tail -n 1\n",
    "        prob = float(out[0].split()[5])\n",
    "        probs[i] = prob\n",
    "    print 'Finish extracting conservativiness for test!'\n",
    "    \n",
    "    # Make predictions\n",
    "    conservativinesses = np.array([ [prob*sentiment] for prob, sentiment in zip(probs, sentiments[test]) ])\n",
    "    predictions = clf.predict(conservativinesses)\n",
    "    \n",
    "    # Record statistics\n",
    "    precision, recall, f_measure, _ = sklearn.metrics.precision_recall_fscore_support(ys[test], predictions, average='binary')\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f_measures.append(f_measure)\n",
    "    \n",
    "    print 'Precision: {}'.format(precision)\n",
    "    print 'Recall: {}'.format(precision)\n",
    "    print 'F-Measure: {}'.format(precision)\n",
    "    \n",
    "print 'Precisions: {}'.format(precisions)\n",
    "print 'Recalls: {}'.format(recalls)\n",
    "print 'F-Measures: {}'.format(f_measures)\n",
    "print\n",
    "print 'Mean Precision: {}'.format(np.mean(precisions))\n",
    "print 'Mean Recall: {}'.format(np.mean(recalls))\n",
    "print 'Mean F-Measure: {}'.format(np.mean(f_measures))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
