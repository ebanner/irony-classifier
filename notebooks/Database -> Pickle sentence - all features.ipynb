{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/v/filer4b/v20q001/npockrus/NLP/finalProject/venv/src/irony-classifier/data/conservative/preprocess\n"
     ]
    }
   ],
   "source": [
    "cd /u/npockrus/NLP/finalProject/venv/src/irony-classifier/data/conservative/preprocess/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWD!!!!! /v/filer4b/v20q001/npockrus/NLP/finalProject/venv/src/irony-classifier/data/conservative/preprocess\n",
      "train/dev database path: /u/npockrus/NLP/finalProject/venv/src/irony-classifier/data/ironate-dk3.sqlite\n",
      "test database path: /u/npockrus/NLP/finalProject/venv/src/irony-classifier/data/ironate-dk3.sqlite\n",
      "5019 comments have been labeled by >= 3 people\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/npockrus/NLP/finalProject/venv/src/irony-classifier/lib/annotation_stats.py:64: DeprecationWarning: You passed a bytestring as `filenames`. This will not work on Python 3. Use `cp.read_file()` or switch to using Unicode strings across the board.\n",
      "  config.read(\"irony.ini\")\n"
     ]
    }
   ],
   "source": [
    "import annotation_stats as db\n",
    "import re\n",
    "\n",
    "labeled_comment_ids = db.get_labeled_thrice_comments()\n",
    "conservative_comment_ids = list(set([c_id for c_id in \n",
    "        db.get_all_comments_from_subreddit(\"Conservative\") if c_id in labeled_comment_ids]))\n",
    "#conservative_sentence_ids = get_sentence_ids_for_comments(conservative_comment_ids)\n",
    "collapse_f = lambda lbl_set: 1 if lbl_set.count(1) >=2 else -1\n",
    "conserv_ids, conserv_texts, conserv_lbls = db.get_texts_and_labels_for_sentences(conservative_comment_ids, add_punctuation_features_to_text=False, collapse=collapse_f)\n",
    "sentiments = db.get_sentiments(conserv_ids)\n",
    "\n",
    "emoticon_RE_str = '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)'\n",
    "question_mark_RE_str = '\\?'\n",
    "exclamation_point_RE_str = '\\!'\n",
    "# Any combination of multiple exclamation points and question marks\n",
    "interrobang_RE_str = '[\\?\\!]{2,}'\n",
    "puncts = [''] * len(conserv_texts)\n",
    "\n",
    "for i, sentence in enumerate(conserv_texts):\n",
    "    sentence = sentence.encode('utf-8')\n",
    "    if len(re.findall(r'%s' % emoticon_RE_str, sentence)) > 0:\n",
    "        puncts[i] += \" PUNCxEMOTICON\"\n",
    "    if len(re.findall(r'%s' % exclamation_point_RE_str, sentence)) > 0:\n",
    "        puncts[i] += \" PUNCxEXCLAMATION_POINT\"\n",
    "    if len(re.findall(r'%s' % question_mark_RE_str, sentence)) > 0:\n",
    "        puncts[i] += \" PUNCxQUESTION_MARK\"\n",
    "    if len(re.findall(r'%s' % interrobang_RE_str, sentence)) > 0:\n",
    "        puncts[i] += \" PUNCxINTERROBANG\"\n",
    "\n",
    "    if any([len(s) > 2 and str.isupper(s) for s in sentence.split(\" \")]):\n",
    "        puncts[i] += \" PUNCxUPPERCASE\" \n",
    "    \n",
    "    puncts[i] = puncts[i].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store labels and sentiment only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/v/filer4b/v20q001/npockrus/NLP/finalProject/venv/src/irony-classifier/data/conservative/preprocess\n"
     ]
    }
   ],
   "source": [
    "cd /u/npockrus/NLP/finalProject/venv/src/irony-classifier/data/conservative/preprocess/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('text-sentiment-label.p', 'wb') as f:\n",
    "    pickle.dump({ sentence:{'label': lbl, 'sentiment': sentiment} for sentence, sentiment, lbl in zip(conserv_texts, sentiments, conserv_lbls) }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author, subreddits = [], []\n",
    "for id_ in conserv_ids:\n",
    "    author.append(db.get_user_ids([db._get_comment_id_for_sentence(id_)]))\n",
    "    subreddits.append(db.get_all_previous_subreddits_for_user(author[-1][0][0].decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/v/filer4b/v20q001/npockrus/NLP/finalProject/venv/src/irony-classifier/data/conservative/features/allFeatures\n"
     ]
    }
   ],
   "source": [
    "cd /u/npockrus/NLP/finalProject/venv/src/irony-classifier/data/conservative/features/allFeatures/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('features.p', 'wb') as f:\n",
    "    pickle.dump({ sentence:{'label': label, 'subreddits': subreddit, 'sentiment': sentiment, 'punctuation':punct} for sentence, label, subreddit, sentiment, punct in zip(conserv_texts, conserv_lbls, subreddits, sentiments, puncts) }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949\n",
      "[u'politics', u'pics', u'canada', u'worldnews', u'news', u'AskReddit', u'WTF', u'cordcutters', u'HistoryPorn', u'Calgary', u'occupywallstreet', u'Frugal', u'videos', u'worldpolitics', u'JusticePorn', u'atheism', u'funny', u'TrueAtheism', u'minimalism', u'technology', u'Quebec', u'dataisbeautiful', u'economy', u'MMA', u'ThriftStoreHauls', u'aww', u'personalfinance', u'nottheonion', u'nononono', u'iphone', u'Military', u'LearnUselessTalents', u'TheFutureIsNow', u'Charlotte', u'Justrolledintotheshop', u'electricvehicles', u'ChevyVolt', u'conspiracy', u'Scams', u'USNEWS']\n",
      "[(u'wazzel2u',)]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print len(subreddits)\n",
    "print subreddits[0]\n",
    "print author[0]\n",
    "print conservative_comment_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
